{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0d44eab",
   "metadata": {},
   "source": [
    "# Token Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ce9039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85601e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z AI API Key exists and begins 616b9f52\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "z_ai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if z_ai_api_key:\n",
    "    print(f\"Z AI API Key exists and begins {z_ai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Z AI API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b71dffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "zai_url=\"https://api.z.ai/api/coding/paas/v4\"\n",
    "zai = OpenAI(api_key=z_ai_api_key, base_url=zai_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32ae9aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = [{\"role\": \"user\", \"content\": \"In Hamlet, when Laertes asks 'Where is my father?' what is the reply?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4f32466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "In Act IV, Scene 5 of *Hamlet*, when Laertes bursts into the castle and confronts King Claudius, he demands:\n",
       "\n",
       "> **Laertes:** Where is my father?\n",
       "\n",
       "King Claudius replies with a single, stark and shocking word:\n",
       "\n",
       "> **Claudius:** **Dead.**\n",
       "\n",
       "This blunt reply is incredibly dramatic for a few reasons:\n",
       "\n",
       "1.  **It's a Political Risk:** Claudius is usually a master of manipulation and speaks in a more roundabout, political way. A single, brutal word is uncharacteristic and could easily enrage Laertes further.\n",
       "2.  **It's Designed to Shock:** Claudius is trying to stun the volatile Laertes into a moment of stillness, cutting through his rage with the stark, awful truth.\n",
       "3.  **It's an Immediate Deflection:** Right after saying \"Dead,\" Claudius immediately points the finger at Hamlet. When Laertes asks, \"But not by him?\" (meaning Hamlet), Claudius replies, \"Ask him yourself.\" This is the crucial moment where Claudius begins to manipulate Laertes's grief and rage for his own purposes, setting the plot for the final duel in motion."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = zai.chat.completions.create(model=\"GLM-4.6\", messages=question, reasoning_effort=\"high\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83c7cd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 27\n",
      "Output tokens: 1500\n",
      "Total tokens: 1527\n",
      "Cached tokens: 0\n",
      "Estimated cost: $0.00316620 ($0.316620 cents)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "\n",
    "# Cached tokens (if provided)\n",
    "cached = 0\n",
    "if hasattr(response.usage, \"prompt_tokens_details\") and response.usage.prompt_tokens_details:\n",
    "    cached = getattr(response.usage.prompt_tokens_details, \"cached_tokens\", 0)\n",
    "    print(f\"Cached tokens: {cached}\")\n",
    "else:\n",
    "    print(\"Cached tokens: N/A\")\n",
    "\n",
    "# Cost calculation for Z.ai GLM-4.6\n",
    "\n",
    "# Use the published rates:\n",
    "rate = {\n",
    "    \"input\": 0.6 / 1_000_000,          # per token (non-cached)\n",
    "    \"output\": 2.1 / 1_000_000,         # per token output\n",
    "    \"cached_input\": 0.11 / 1_000_000   # per token cached input\n",
    "}\n",
    "\n",
    "inp = response.usage.prompt_tokens\n",
    "out = response.usage.completion_tokens\n",
    "\n",
    "# Let non_cached_input = inp - cached (tokens actually processed)\n",
    "non_cached_input = max(inp - cached, 0)\n",
    "\n",
    "# Compute cost\n",
    "cost = non_cached_input * rate[\"input\"] \\\n",
    "     + cached * rate[\"cached_input\"] \\\n",
    "     + out * rate[\"output\"]\n",
    "\n",
    "print(f\"Estimated cost: ${cost:.8f} (${cost*100:.6f} cents)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b179559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "That is an excellent question that gets to the heart of one of the most dramatic moments in the play.\n",
       "\n",
       "The short answer is that **King Claudius does not give Laertes a direct, simple reply.** Instead of saying, \"He is dead,\" Claudius tries to calm the furious Laertes and then subtly manipulates his anger.\n",
       "\n",
       "### The Detailed Answer\n",
       "\n",
       "Laertes bursts into the castle in Act 4, Scene 5, having returned from France upon hearing of his father Polonius's death and his sister Ophelia's madness. He confronts King Claudius directly.\n",
       "\n",
       "While your quote, \"Where is my father?\" perfectly captures the spirit of his demand, his actual line in the play is:\n",
       "\n",
       "> **LAERTES:** \"O thou vile king,\n",
       "> Give me my father!\"\n",
       "\n",
       "Claudius's immediate reply is an attempt to de-escalate the situation, not to answer the question:\n",
       "\n",
       "> **CLAUDIUS:** \"Peace, lady, or the shower that follows will be a tempest.\"\n",
       "\n",
       "Laertes is not satisfied and continues to rage, demanding to know who is responsible for his father's death. Claudius, seeing an opportunity, finally gives Laertes the answer he's looking for, but frames it in a way that serves his own purposeâ€”to turn Laertes against Hamlet.\n",
       "\n",
       "The true \"reply\" comes when Claudius confirms that **Hamlet is the killer** and explains why he has not yet been punished. This happens a bit later in the same scene and continues into Act 4, Scene 7. Claudius tells Laertes:\n",
       "\n",
       "> \"Hamlet is the one who did it. And for his offense, the law is clear... But so much was my love for you, and for the queen his mother, that I could not act. It was like the owner of a foul disease, who, to keep it from being discovered, lets it feed even on the pith of life.\"\n",
       "\n",
       "In essence, Claudius's reply is:\n",
       "\n",
       "1.  **Evasion:** He first tries to calm Laertes down without giving a straight answer.\n",
       "2.  **Accusation:** He then confirms that Hamlet killed Polonius.\n",
       "3.  **Manipulation:** He spins his inaction as a favor to Laertes, claiming he protected Hamlet because of his love for the Queen and fear of public backlash, all while secretly plotting with Laertes to murder Hamlet.\n",
       "\n",
       "So, while there is no single, memorable line like \"He is dead and buried,\" the reply is a masterclass in political manipulation, where Claudius transforms a grieving son into an instrument of his own revenge."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = zai.chat.completions.create(model=\"GLM-4.6\", messages=question, reasoning_effort=\"high\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "023f6094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 27\n",
      "Output tokens: 3105\n",
      "Total tokens: 3132\n",
      "Cached tokens: 0\n",
      "Estimated cost: $0.00653670 ($0.653670 cents)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "\n",
    "# Cached tokens (if provided)\n",
    "cached = 0\n",
    "if hasattr(response.usage, \"prompt_tokens_details\") and response.usage.prompt_tokens_details:\n",
    "    cached = getattr(response.usage.prompt_tokens_details, \"cached_tokens\", 0)\n",
    "    print(f\"Cached tokens: {cached}\")\n",
    "else:\n",
    "    print(\"Cached tokens: N/A\")\n",
    "\n",
    "# Cost calculation for Z.ai GLM-4.6\n",
    "\n",
    "# Use the published rates:\n",
    "rate = {\n",
    "    \"input\": 0.6 / 1_000_000,          # per token (non-cached)\n",
    "    \"output\": 2.1 / 1_000_000,         # per token output\n",
    "    \"cached_input\": 0.11 / 1_000_000   # per token cached input\n",
    "}\n",
    "\n",
    "inp = response.usage.prompt_tokens\n",
    "out = response.usage.completion_tokens\n",
    "\n",
    "# Let non_cached_input = inp - cached (tokens actually processed)\n",
    "non_cached_input = max(inp - cached, 0)\n",
    "\n",
    "# Compute cost\n",
    "cost = non_cached_input * rate[\"input\"] \\\n",
    "     + cached * rate[\"cached_input\"] \\\n",
    "     + out * rate[\"output\"]\n",
    "\n",
    "print(f\"Estimated cost: ${cost:.8f} (${cost*100:.6f} cents)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "556da24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google API Key exists and begins AI\n"
     ]
    }
   ],
   "source": [
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84fed0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a096af80",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini = OpenAI(api_key=google_api_key, base_url=gemini_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae106c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b6cd4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak, man.\n",
      "  Laer. Where is my father?\n",
      "  King. Dead.\n",
      "  Queen. But not by him!\n",
      "  King. Let him deman\n"
     ]
    }
   ],
   "source": [
    "with open(\"hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hamlet = f.read()\n",
    "\n",
    "loc = hamlet.find(\"Speak, man\")\n",
    "print(hamlet[loc:loc+100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6df574ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = [{\"role\": \"user\", \"content\": \"In Hamlet, when Laertes asks 'Where is my father?' what is the reply?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a7161d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In Shakespeare's *Hamlet*, when Laertes returns from France and learns of his father Polonius's death, he is distraught and enraged. He bursts into the castle demanding to know where his father is.\n",
       "\n",
       "The reply he receives is from **Claudius**.\n",
       "\n",
       "Claudius, trying to control the situation and appear sympathetic, tells him:\n",
       "\n",
       "\"**He is dead.**\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = completion(model=\"gemini/gemini-2.5-flash-lite\", messages=question)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5678df98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 19\n",
      "Output tokens: 79\n",
      "Cached tokens: None\n",
      "Total cost: 0.0033 cents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Cached tokens: {response.usage.prompt_tokens_details.cached_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params['response_cost']*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56f43d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In Hamlet, when Laertes asks \"Where is my father?\", the reply comes from **Gertrude**, who says:\n",
       "\n",
       "**\"He is sad, my lord.\"**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = completion(model=\"gemini/gemini-2.5-flash-lite\", messages=question)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f5da290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 19\n",
      "Output tokens: 35\n",
      "Cached tokens: None\n",
      "Total cost: 0.0016 cents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Cached tokens: {response.usage.prompt_tokens_details.cached_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params['response_cost']*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c4c767",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_engineering_practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
