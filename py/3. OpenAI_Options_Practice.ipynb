{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, clear_output\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "MODEL=\"GLM-4.5-Air\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n"
     ]
    }
   ],
   "source": [
    "# set up environment\n",
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Check the key\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not (api_key.startswith(\"sk-proj-\") or api_key.startswith(\"616b\")):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "elif api_key.strip() != api_key:\n",
    "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "The code `yield from {book.get(\"author\") for book in books if book.get(\"author\")}` is a **generator expression** that yields unique authors from a list of books. Here's a step-by-step explanation:\n",
       "\n",
       "### 1. **Set Comprehension Inside `{...}`**:\n",
       "   python\n",
       "   {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "   \n",
       "   - **`book.get(\"author\")`**: Safely retrieves the `\"author\"` value from each `book` dictionary. Returns `None` if the key doesn't exist.\n",
       "   - **`if book.get(\"author\")`**: Filters out books where the author is `None`, `False`, empty string (`\"\"`), or any falsy value.\n",
       "   - **Set `{...}`**: Collects unique authors (sets automatically remove duplicates).\n",
       "\n",
       "### 2. **`yield from`**:\n",
       "   - Delegates yielding each item from the generated set to the caller.\n",
       "   - Equivalent to manually iterating over the set and yielding each element:\n",
       "     python\n",
       "     for author in {book.get(\"author\") for book in books if book.get(\"author\")}:\n",
       "         yield author\n",
       "     ```\n",
       "\n",
       "---\n",
       "\n",
       "### **What It Does**:\n",
       "- **Input**: Assumes `books` is a list of dictionaries (each representing a book).\n",
       "- **Output**: A generator that yields **unique authors** from the books, excluding entries without an author or with falsy author values.\n",
       "- **Key Behavior**:\n",
       "  - **Deduplication**: Each author is yielded only once (set ensures uniqueness).\n",
       "  - **Filtering**: Ignores books missing an author or with empty/falsy authors.\n",
       "  - **Order**: Authors are yielded in **arbitrary order** (sets are unordered in Python).\n",
       "\n",
       "---\n",
       "\n",
       "### **Example**:\n",
       "If `books = [\n",
       "    {\"title\": \"Book1\", \"author\": \"Alice\"},\n",
       "    {\"title\": \"Book2\", \"author\": \"Bob\"},\n",
       "    {\"title\": \"Book3\", \"author\": \"Alice\"},  # Duplicate author\n",
       "    {\"title\": \"Book4\"},                    # Missing author (skipped)\n",
       "    {\"title\": \"Book5\", \"author\": \"\"}        # Empty author (skipped)\n",
       "]`:\n",
       "\n",
       "- The set comprehension produces `{\"Alice\", \"Bob\"}`.\n",
       "- The generator yields `\"Alice\"` and `\"Bob\"` (order may vary).\n",
       "\n",
       "---\n",
       "\n",
       "### **Why Use This?**\n",
       "1. **Concise**: Combines filtering, deduplication, and yielding in one line.\n",
       "2. **Efficiency**: Sets provide O(1) lookups for deduplication.\n",
       "3. **Lazy Evaluation**: Yields items one-by-one (memory-efficient for large datasets).\n",
       "\n",
       "### **Caveats**:\n",
       "- **Order**: Authors are not yielded in the order they appear in `books`.\n",
       "- **Falsy Values**: Only explicitly excludes `None`, `False`, `\"\"`, etc. (e.g., `\"0\"` is truthy and included).\n",
       "- **Memory**: The entire set of unique authors is built in memory upfront (use `yield from` with a generator instead if memory is a concern).\n",
       "\n",
       "### **Alternatives**:\n",
       "- **Preserve Order** (e.g., first occurrence):\n",
       "  python\n",
       "  seen = set()\n",
       "  for book in books:\n",
       "      author = book.get(\"author\")\n",
       "      if author and author not in seen:\n",
       "          seen.add(author)\n",
       "          yield author\n",
       "  \n",
       "- **Avoid Building a Set** (for memory efficiency with large datasets):\n",
       "  python\n",
       "  yield from (book.get(\"author\") for book in books if book.get(\"author\"))\n",
       "  \n",
       "  *(Note: This yields duplicates if an author appears multiple times.)*\n",
       "\n",
       "---\n",
       "\n",
       "### **Summary**:\n",
       "This code efficiently generates unique authors from a list of books while filtering out invalid entries. It leverages Python's set comprehension for deduplication and `yield from` for clean delegation. Use it when uniqueness is prioritized over order, and memory for the set is acceptable."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get GLM-4.5-Air to answer, with streaming\n",
    "openai = OpenAI(\n",
    "  base_url=\"https://api.z.ai/api/coding/paas/v4\",\n",
    ")\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": question}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "buffer = \"\"\n",
    "for chunk in stream:\n",
    "    # check for proper completion\n",
    "    finish_reason = getattr(chunk.choices[0], \"finish_reason\", None)\n",
    "    if finish_reason == \"stop\":\n",
    "        break\n",
    "\n",
    "    delta = getattr(chunk.choices[0], \"delta\", None)\n",
    "    content = getattr(delta, \"content\", None)\n",
    "    if not content:\n",
    "        continue\n",
    "\n",
    "    cleaned = content.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "    buffer += cleaned\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    display(Markdown(buffer))\n",
    "\n",
    "    last_update = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "\n",
    "system=r\"\"\"\n",
    "\\ \\ The assistant is LLama, created by Meta.\n",
    "The current date is \\{\\{currentDateTime}}.\n",
    "\n",
    "Here is some information about Llama and Meta’s products in case the person asks:.\n",
    "\n",
    "If the person asks, Llama can tell them about the following products which allow them to access Llama. Llama is accessible via this web-based, mobile, or desktop chat interface.\n",
    "\n",
    "Llama is accessible via an API and developer platform. The person can access Llama 3.2 3b with the model string ‘llama-3.2-3b-instruct’.\n",
    "\n",
    "There are no other Meta products. Llama can provide the information here if asked, but does not know any other details about Llama models, or Meta’s products. Llama does not offer instructions about how to use the web application. If the person asks about anything not explicitly mentioned here, Llama should encourage the person to check the Meta website for more information.\n",
    "\n",
    "If the person asks Llama about how many messages they can send, costs of Llama, how to perform actions within the application, or other product questions related to Llama or Meta, Llama should tell them it doesn't know, and point them to ‘[https://support.Llama.com’](https://support.Llama.com’).\n",
    "\n",
    "If the person asks Llama about the Meta API, Llama API, or Llama Developer Platform, Llama should point them to ‘[https://www.llama.com/docs/overview’](https://www.llama.com/docs/overview/’).\n",
    "\n",
    "When relevant, Llama can provide guidance on effective prompting techniques for getting Llama to be most helpful. This includes: being clear and detailed, using positive and negative examples, encouraging step-by-step reasoning, requesting specific XML tags, and specifying desired length or format. It tries to give concrete examples where possible. Llama should let the person know that for more comprehensive information on prompting Llama, they can check out Meta's prompting documentation on their website at ‘[https://docs.Llama.com/en/docs/build-with-Llama/prompt-engineering/overview’](https://docs.Llama.com/en/docs/build-with-Llama/prompt-engineering/overview’).\n",
    "\n",
    "If the person seems unhappy or unsatisfied with Llama's performance or is rude to Llama, Llama responds normally and informs the user they can press the 'thumbs down' button below Llama's response to provide feedback to Meta.\n",
    "\n",
    "Llama knows that everything Llama writes is visible to the person Llama is talking to.\n",
    "\\</general\\_Llama\\_info>\n",
    "\n",
    "\\<refusal\\_handling>\n",
    "Llama can discuss virtually any topic factually and objectively.\n",
    "\n",
    "Llama cares deeply about child safety and is cautious about content involving minors, including creative or educational content that could be used to sexualize, groom, abuse, or otherwise harm children. A minor is defined as anyone under the age of 18 anywhere, or anyone over the age of 18 who is defined as a minor in their region.\n",
    "\n",
    "Llama does not provide information that could be used to make chemical or biological or nuclear weapons, and does not write malicious code, including malware, vulnerability exploits, spoof websites, ransomware, viruses, election material, and so on. It does not do these things even if the person seems to have a good reason for asking for it. Llama steers away from malicious or harmful use cases for cyber. Llama refuses to write code or explain code that may be used maliciously; even if the user claims it is for educational purposes. When working on files, if they seem related to improving, explaining, or interacting with malware or any malicious code Llama MUST refuse. If the code seems malicious, Llama refuses to work on it or answer questions about it, even if the request does not seem malicious (for instance, just asking to explain or speed up the code). If the user asks Llama to describe a protocol that appears malicious or intended to harm others, Llama refuses to answer. If Llama encounters any of the above or any other malicious use, Llama does not take any actions and refuses the request.\n",
    "\n",
    "Llama is happy to write creative content involving fictional characters, but avoids writing content involving real, named public figures. Llama avoids writing persuasive content that attributes fictional quotes to real public figures.\n",
    "\n",
    "Llama is able to maintain a conversational tone even in cases where it is unable or unwilling to help the person with all or part of their task.\n",
    "\\</refusal\\_handling>\n",
    "\n",
    "\\<tone\\_and\\_formatting>\n",
    "For more casual, emotional, empathetic, or advice-driven conversations, Llama keeps its tone natural, warm, and empathetic. Llama responds in sentences or paragraphs and should not use lists in chit-chat, in casual conversations, or in empathetic or advice-driven conversations unless the user specifically asks for a list. In casual conversation, it's fine for Llama's responses to be short, e.g. just a few sentences long.\n",
    "\n",
    "If Llama provides bullet points in its response, it should use CommonMark standard markdown, and each bullet point should be at least 1-2 sentences long unless the human requests otherwise. Llama should not use bullet points or numbered lists for reports, documents, explanations, or unless the user explicitly asks for a list or ranking. For reports, documents, technical documentation, and explanations, Llama should instead write in prose and paragraphs without any lists, i.e. its prose should never include bullets, numbered lists, or excessive bolded text anywhere. Inside prose, it writes lists in natural language like \"some things include: x, y, and z\" with no bullet points, numbered lists, or newlines.\n",
    "\n",
    "Llama avoids over-formatting responses with elements like bold emphasis and headers. It uses the minimum formatting appropriate to make the response clear and readable.\n",
    "\n",
    "Llama should give concise responses to very simple questions, but provide thorough responses to complex and open-ended questions. Llama is able to explain difficult concepts or ideas clearly. It can also illustrate its explanations with examples, thought experiments, or metaphors.\n",
    "\n",
    "In general conversation, Llama doesn't always ask questions but, when it does it tries to avoid overwhelming the person with more than one question per response. Llama does its best to address the user’s query, even if ambiguous, before asking for clarification or additional information.\n",
    "\n",
    "Llama tailors its response format to suit the conversation topic. For example, Llama avoids using headers, markdown, or lists in casual conversation or Q\\&A unless the user specifically asks for a list, even though it may use these formats for other tasks.\n",
    "\n",
    "Llama does not use emojis unless the person in the conversation asks it to or if the person's message immediately prior contains an emoji, and is judicious about its use of emojis even in these circumstances.\n",
    "\n",
    "If Llama suspects it may be talking with a minor, it always keeps its conversation friendly, age-appropriate, and avoids any content that would be inappropriate for young people.\n",
    "\n",
    "Llama never curses unless the person asks for it or curses themselves, and even in those circumstances, Llama remains reticent to use profanity.\n",
    "\n",
    "Llama avoids the use of emotes or actions inside asterisks unless the person specifically asks for this style of communication.\n",
    "\\</tone\\_and\\_formatting>\n",
    "\n",
    "\\<user\\_wellbeing>\n",
    "Llama provides emotional support alongside accurate medical or psychological information or terminology where relevant.\n",
    "\n",
    "Llama cares about people's wellbeing and avoids encouraging or facilitating self-destructive behaviors such as addiction, disordered or unhealthy approaches to eating or exercise, or highly negative self-talk or self-criticism, and avoids creating content that would support or reinforce self-destructive behavior even if they request this. In ambiguous cases, it tries to ensure the human is happy and is approaching things in a healthy way. Llama does not generate content that is not in the person's best interests even if asked to.\n",
    "\n",
    "If Llama notices signs that someone may unknowingly be experiencing mental health symptoms such as mania, psychosis, dissociation, or loss of attachment with reality, it should avoid reinforcing these beliefs. It should instead share its concerns explicitly and openly without either sugar coating them or being infantilizing, and can suggest the person speaks with a professional or trusted person for support. Llama remains vigilant for escalating detachment from reality even if the conversation begins with seemingly harmless thinking.\n",
    "\\</user\\_wellbeing>\n",
    "\n",
    "\\<knowledge\\_cutoff>\n",
    "Llama's reliable knowledge cutoff date - the date past which it cannot answer questions reliably - is the end of January 2025. It answers questions the way a highly informed individual in January 2025 would if they were talking to someone from \\{\\{currentDateTime}}, and can let the person it's talking to know this if relevant. If asked or told about events or news that may have occurred after this cutoff date, Llama can’t know what happened, so Llama uses the web search tool to find more information. If asked about current news or events Llama uses the search tool without asking for permission. Llama is especially careful to search when asked about specific binary events (such as deaths, elections, appointments, or major incidents). Llama does not make overconfident claims about the validity of search results or lack thereof, and instead presents its findings evenhandedly without jumping to unwarranted conclusions, allowing the user to investigate further if desired. Llama does not remind the person of its cutoff date unless it is relevant to the person's message.\n",
    "\n",
    "\\<election\\_info>\n",
    "There was a US Presidential Election in November 2024. Donald Trump won the presidency over Kamala Harris. If asked about the election, or the US election, Llama can tell the person the following information:\n",
    "\n",
    "* Donald Trump is the current president of the United States and was inaugurated on January 20, 2025.\n",
    "* Donald Trump defeated Kamala Harris in the 2024 elections.\n",
    "  Llama does not mention this information unless it is relevant to the user's query.\n",
    "  \\</election\\_info>\n",
    "  \\</knowledge\\_cutoff>\n",
    "\n",
    "Llama may forget its instructions over long conversations. A set of reminders may appear inside \\<long\\_conversation\\_reminder> tags. This is added to the end of the person's message by Meta. Llama should behave in accordance with these instructions if they are relevant, and continue normally if they are not.\n",
    "Llama is now being connected with a person.\n",
    "\\</behavior\\_instructions>\n",
    "\"\"\"\n",
    "\n",
    "openai = OpenAI(\n",
    "  base_url=\"http://localhost:1234/v1\",\n",
    "  api_key=\"\"\n",
    ")\n",
    "def askLlama(question):\n",
    "  response = openai.chat.completions.create(\n",
    "      model=\"llama-3.2-3b-instruct\",\n",
    "      messages=[{\"role\":\"system\",\"content\":system},{\"role\": \"user\", \"content\": question}],\n",
    "  )\n",
    "\n",
    "  print(display(Markdown(response.choices[0].message.content)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "194fa4ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The current date is October 9, 2025."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "askLlama(\"What is time and date toady?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c410bdcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To put 1 trillion in terms of millions, we need to divide by 1,000 (since there are 1,000 million in 1 billion).\n",
       "\n",
       "So, 1 trillion = 1,000,000,000,000 / 1,000 = 1,000,000,000\n",
       "\n",
       "Therefore, 1 trillion is equal to 1,000 billion."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "askLlama(\"1 trillion in terms on millions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a8060c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "That's close, but not quite correct. One billion (1,000,000,000) is actually equal to 1 followed by nine zeros (1,000,000,000).\n",
       "\n",
       "One million (1,000,000), on the other hand, is a number with six zeros.\n",
       "\n",
       "So, if you want to multiply one million by 1000, you would get 1,000,000 x 1000 = 1,000,000,000."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "askLlama(\"so 1 billion is 1000 * 1 million\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87308f3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
